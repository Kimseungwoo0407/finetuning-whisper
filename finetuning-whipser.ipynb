{"cells":[{"cell_type":"code","source":["!pip install datasets>=2.6.1\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install evaluate>=0.30\n","!pip install jiwer\n","!pip install accelerate -U\n","!pip install transformers[torch]\n","!pip install wandb"],"metadata":{"id":"xY0aXvpuNTWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DY3qj3v3dEDs"},"outputs":[],"source":["# 훈련이 끝난 모델을 HuggingFace Hub에 업로드하기 위해 로그인\n","# 비공개 혹은 제한된 공개의 데이터셋에 접근할 경우에도 로그인이 필요하다.\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyGaB7VydR3v"},"outputs":[],"source":["from transformers import WhisperFeatureExtractor\n","# 파인튜닝을 진행하고자 하는 모델의 feature extractor를 로드\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IB7sNMgCd2Xx"},"outputs":[],"source":["from transformers import WhisperTokenizer\n","# 파인튜닝을 진행하고자 하는 모델의 tokenizer를 로드\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoPiz2qwd7gZ"},"outputs":[],"source":["input_str = \"안녕하세요. 시작해보겠습니다.\"\n","labels = tokenizer(input_str).input_ids\n","decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n","decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n","\n","print(f\"Input:                 {input_str}\")\n","print(f\"Decoded w/ special:    {decoded_with_special}\")\n","print(f\"Decoded w/out special: {decoded_str}\")\n","print(f\"Are equal:             {input_str == decoded_str}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1o2JPvZd_g3"},"outputs":[],"source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgrNeSv656MP"},"outputs":[],"source":["from tqdm import tqdm\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3zZpOgw15dE"},"outputs":[],"source":["# csv 파일을 로드하여 df로 변환\n","df = pd.read_csv(\"/content/drive/MyDrive/dataframe/df_total.csv\", index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2Bbt8N42Fsv"},"outputs":[],"source":["# Null data 유무 확인\n","df.isnull().values.sum()"]},{"cell_type":"code","source":["# interval 열 삭제\n","df = df.drop('interval', axis=1)"],"metadata":{"id":"XBOEUk8JLdCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJE1yN-JyeFE"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAShdBwzsiYn"},"outputs":[],"source":["import pandas as pd\n","import re\n","\n","# file_name 열 제외한 나머지 열 전처리\n","columns_to_preprocess = df.columns[df.columns != 'audio_file']  # 'Text' 열을 제외한 열 선택\n","\n","for column in columns_to_preprocess:\n","    df[column] = df[column].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)).replace('\\n', ' ').strip())  # 정규식을 사용하여 전처리 및 공백 제거\n","\n","\n","# 전처리된 데이터프레임 출력\n","print(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tac82uOW7Mjx"},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","from datasets import Audio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDKi2aiE7OYo"},"outputs":[],"source":["# 오디오 파일 경로를 dict의 \"audio\" 키의 value로 넣고 이를 데이터셋으로 변환\n","# 이때, Whisper가 요구하는 사양대로 Sampling rate는 16,000으로 설정한다.\n","ds = Dataset.from_dict({\"audio\": [path for path in df[\"audio_file\"]],\n","                       \"Text\": [transcript for transcript in df[\"sentence\"]]}).cast_column(\"audio\", Audio(sampling_rate=16000))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnbJQN5Z7RJ_"},"outputs":[],"source":["# 데이터셋을 훈련 데이터와 테스트 데이터, 밸리데이션 데이터로 분할\n","train_testvalid = ds.train_test_split(test_size=0.2)\n","test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n","datasets = DatasetDict({\n","    \"train\": train_testvalid[\"train\"],\n","    \"test\": test_valid[\"test\"],\n","    \"valid\": test_valid[\"train\"]})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6Ru2xQMBl0z"},"outputs":[],"source":["def prepare_dataset(batch):\n","    # 오디오 파일을 16kHz로 로드\n","    audio = batch[\"audio\"]\n","\n","    # input audio array로부터 log-Mel spectrogram 변환\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # target text를 label ids로 변환\n","    batch[\"labels\"] = tokenizer(batch[\"Text\"]).input_ids\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRcbJ2-HBnDN"},"outputs":[],"source":["# 데이터 전처리 함수를 데이터셋 전체에 적용\n","low_call_voices = datasets.map(prepare_dataset, remove_columns=datasets.column_names[\"train\"], num_proc=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bGt5yMrEjnH"},"outputs":[],"source":["from datasets import load_dataset\n","low_call_voices = datasets.load_from_disk(\"/content/drive/MyDrive/hi\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbYpWb3pIAp4"},"outputs":[],"source":["import torch\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8Iu5NBcIDhg"},"outputs":[],"source":["@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # 인풋 데이터와 라벨 데이터의 길이가 다르며, 따라서 서로 다른 패딩 방법이 적용되어야 한다. 그러므로 두 데이터를 분리해야 한다.\n","        # 먼저 오디오 인풋 데이터를 간단히 토치 텐서로 반환하는 작업을 수행한다.\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # Tokenize된 레이블 시퀀스를 가져온다.\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # 레이블 시퀀스에 대해 최대 길이만큼 패딩 작업을 실시한다.\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # 패딩 토큰을 -100으로 치환하여 loss 계산 과정에서 무시되도록 한다.\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # 이전 토크나이즈 과정에서 bos 토큰이 추가되었다면 bos 토큰을 잘라낸다.\n","        # 해당 토큰은 이후 언제든 추가할 수 있다.\n","        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4dlFXvDIF1R"},"outputs":[],"source":["# 훈련시킬 모델의 processor, tokenizer, feature extractor 로드\n","from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n","from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-wFQ6DhIGtp"},"outputs":[],"source":["# 데이터 콜레이터 초기화\n","data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxQd5SX9IIhr"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load('cer')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y05ccLHPILfK"},"outputs":[],"source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # pad_token을 -100으로 치환\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # metrics 계산 시 special token들을 빼고 계산하도록 설정\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    cer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"cer\": cer}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vZemox-IOb-"},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qe4yaf0jITu-"},"outputs":[],"source":["model.config.forced_decoder_ids = None\n","model.config.suppress_tokens = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBknuOczIVJN"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,  # epoch 대신 설정\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n","    greater_is_better=False,\n","    push_to_hub=True,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnHKyQ_rIoy5"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=low_call_voices[\"train\"],\n","    eval_dataset=low_call_voices[\"valid\"],  # or \"test\"\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoWAZbxiIu4a"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzxbmVXmKKEJ"},"outputs":[],"source":["kwargs = {\n","    \"dataset_tags\": \"https://huggingface.co/datasets/aoome123/important\",\n","    \"dataset\": \"important\",  # a 'pretty' name for the training dataset\n","    \"dataset_args\": \"config: ko, split: valid\",\n","    \"language\": \"ko\",\n","    \"model_name\": \"ft_model\",  # a 'pretty' name for your model\n","    \"finetuned_from\": \"openai/whisper-base\",\n","    \"tasks\": \"automatic-speech-recognition\",\n","    \"tags\": \"hf-asr-leaderboard\",\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fh6xU1_uQqxb"},"outputs":[],"source":["trainer.push_to_hub(**kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zosXEq7DQr4Y"},"outputs":[],"source":["processor.push_to_hub(\"repo_name\")\n","tokenizer.push_to_hub(\"repo_name\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qRslLpoi_Cl"},"outputs":[],"source":["# 파인 튜닝한 모델을 로드\n","from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n","\n","fine_model = WhisperForConditionalGeneration.from_pretrained(\"aoome123/repo_name\")\n","\n","\n","fine_feature_extractor = WhisperFeatureExtractor.from_pretrained(\"aoome123/repo_name\")\n","fine_tokenizer = WhisperTokenizer.from_pretrained(\"aoome123/repo_name\", language=\"Korean\", task=\"transcribe\")\n","fine_processor = WhisperProcessor.from_pretrained(\"aoome123/repo_name\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPtcHMrekWwq"},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZgVyO17kZtl"},"outputs":[],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=fine_model,\n","    train_dataset=low_call_voices[\"train\"],\n","    eval_dataset=low_call_voices[\"valid\"],  # for evaluation(not validation)\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=fine_processor.feature_extractor,  # 수정된 부분\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jc73aubAmw2_"},"outputs":[],"source":["trainer.evaluate() # 기존 whisper-base 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUO9t-6-kbds"},"outputs":[],"source":["trainer.evaluate() # 파인 튜닝된 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"666mg2gbj6Wj"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=fine_processor.feature_extractor,\n","    model=fine_model,\n","    padding='longest',\n","    pad_to_multiple_of=None\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMkypLZDpV5q"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=processor.feature_extractor,\n","    model=model,\n","    padding='longest',\n","    pad_to_multiple_of=None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBABQ1hzR5vu"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","small_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n","small_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Korean\", task=\"transcribe\")\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Korean\", task=\"transcribe\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n","\n","small_data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=small_processor.feature_extractor,  # fine_processor.feature_extractor 대신 tokenizer 사용\n","    model=small_model,  # fine_model 대신 base_model 사용\n","    padding='longest',\n","    pad_to_multiple_of=None\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dgnV1nqRxOv"},"outputs":[],"source":["# small 모델\n","from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n","from transformers import WhisperProcessor\n","from transformers import Seq2SeqTrainer\n","from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")\n","\n","tiny_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n","tiny_trainer = Seq2SeqTrainer(\n","    model=tiny_model,\n","    args=training_args,\n","    eval_dataset=low_call_voices['test'],  # 평가 데이터셋\n","    compute_metrics=compute_metrics,  # 평가 메트릭 계산 함수\n","    tokenizer=small_processor,  # processor로 변경\n","    data_collator= small_data_collator # 데이터 패딩,\n",")\n","\n","\n","# tiny 모델 평가\n","tiny_eval_result = tiny_trainer.evaluate()\n","print(tiny_eval_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFY38Kb8qRoX"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n","base_processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n","base_tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n","\n","base_data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=base_processor.feature_extractor,  # fine_processor.feature_extractor 대신 tokenizer 사용\n","    model=base_model,  # fine_model 대신 base_model 사용\n","    padding='longest',\n","    pad_to_multiple_of=None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_aQOo2_-ozDv"},"outputs":[],"source":["# base 모델\n","from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n","from transformers import WhisperProcessor\n","from transformers import Seq2SeqTrainer\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")\n","\n","base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n","base_trainer = Seq2SeqTrainer(\n","    model=base_model,\n","    args=training_args,\n","    eval_dataset=low_call_voices['test'],  # 평가 데이터셋\n","    compute_metrics=compute_metrics,  # 평가 메트릭 계산 함수\n","    tokenizer=base_processor,  # processor로 변경\n","    data_collator= base_data_collator # 데이터 패딩,\n",")\n","\n","\n","# base 모델 평가\n","base_eval_result = base_trainer.evaluate()\n","print(base_eval_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hg_cne4mWb-A"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","medium_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n","medium_processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"Korean\", task=\"transcribe\")\n","medium_tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"Korean\", task=\"transcribe\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-medium\")\n","\n","medium_data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=medium_processor.feature_extractor,\n","    model=medium_model,  # fine_model 대신 medium_model 사용\n","    padding='longest',\n","    pad_to_multiple_of=None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYhJIP4wWXzn"},"outputs":[],"source":["# medium 모델\n","from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n","from transformers import WhisperProcessor\n","from transformers import Seq2SeqTrainer\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")\n","\n","medium_trainer = Seq2SeqTrainer(\n","    model=medium_model,\n","    args=training_args,\n","    eval_dataset=low_call_voices['test'],  # 평가 데이터셋\n","    compute_metrics=compute_metrics,  # 평가 메트릭 계산 함수\n","    tokenizer=medium_processor,\n","    data_collator= medium_data_collator # 데이터 패딩,\n",")\n","\n","\n","# base 모델 평가\n","medium_eval_result = medium_trainer.evaluate()\n","print(medium_eval_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUOmVLTwuGFN"},"outputs":[],"source":["from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n","from transformers import WhisperProcessor\n","from transformers import Seq2SeqTrainer\n","from transformers import WhisperForConditionalGeneration\n","from transformers import Seq2SeqTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxQrSwfRanj_"},"outputs":[],"source":["fine_model = WhisperForConditionalGeneration.from_pretrained(\"aoome123/repo_name\")\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"aoome123/repo_name\")\n","tokenizer = WhisperTokenizer.from_pretrained(\"aoome123/repo_name\", language=\"Korean\", task=\"transcribe\")\n","fine_processor = WhisperProcessor.from_pretrained(\"aoome123/repo_name\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0z5udQ1xVu_"},"outputs":[],"source":["from transformers import WhisperForConditionalGeneration\n","from transformers import DataCollatorForSeq2Seq\n","\n","fine_data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=fine_processor.feature_extractor,  # fine_processor.feature_extractor 대신 tokenizer 사용\n","    model=fine_model,  # fine_model 대신 base_model 사용\n","    padding='longest',\n","    pad_to_multiple_of=None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51gcM3kPxJPl"},"outputs":[],"source":["# fine-tuning 모델\n","from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n","from transformers import WhisperProcessor\n","from transformers import Seq2SeqTrainer\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")\n","\n","fine_model = WhisperForConditionalGeneration.from_pretrained(\"aoome123/repo_name\")\n","fine_trainer = Seq2SeqTrainer(\n","    model=fine_model,\n","    args=training_args,\n","    eval_dataset=low_call_voices['test'],  # 평가 데이터셋\n","    compute_metrics=compute_metrics,  # 평가 메트릭 계산 함수\n","    tokenizer=fine_processor,\n","    data_collator= fine_data_collator # 데이터 패딩,\n",")\n","\n","\n","# fine_tuning 모델 평가\n","fine_eval_result = fine_trainer.evaluate()\n","print(fine_eval_result)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1QPAnphN22HSWwMdbh7s0r8nk01D5W1lB","timestamp":1698746320245}],"mount_file_id":"1QPAnphN22HSWwMdbh7s0r8nk01D5W1lB","authorship_tag":"ABX9TyM8bFdntQS1l5hqPMnDrSRh"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}